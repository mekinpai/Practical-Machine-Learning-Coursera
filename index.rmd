---
title: "Practical Machine Learning Coursera Project"
author: "Mekin Lertanuntasuk"
date: "4/14/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The goal of this analysis is to build machine learning models to predict the manner in which people exercise using the data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. In this report, we are going to describe how we built the models, how we used cross validation, and what is the expected out of sample error.

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

## Summary

Of all the three machine learning models we tried, random forest turned out to be the best with out-of-sample accuracy of 99.8%. Using generalized boosted regression, we can also acheive about 99% accuracy on the test set. Nevertheless, the descision tree turned out to be the worst with less than 50% accuracy.

## Environment setup

First, let's load the required libraries for the analysis

```{r, echo=TRUE}
set.seed(1234)
library(caret)
```

Let's load the training and validation dataset

```{r, echo=TRUE}
training = read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), header=T)
validation = read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), header=T)
```

## Data cleaning and preprocessing

Let's look at how our dataset looks like

```{r, echo=TRUE}
dim(training)
str(training)
```

We can see that there are about 20K records with 160 columns. We can also see that for some columns, there are a lot of NA or blank values. Let's see what those columns are.

```{r, echo=TRUE}
countNA = sapply(training, function(y) sum(length(which(is.na(y)|y==""))))
countNA[countNA>0]
```

We can see that there are 100 columns with mostly NA or blank values. We are going to remove them as they don't provide us any information. Also, we are going to further remove the first five columns as they are just containing the data about the users who did the exercise and the timestamp.

```{r, echo=TRUE}
training = training[, names(countNA[countNA==0])]
training = training[, -c(1:5)]
```

```{r, echo=TRUE}
dim(training)
```

We can see that our data now has only 55 columns.

Now let's split the training data into 70% training and 30% testing for further model building and evaluation.

```{r, echo=TRUE}
inTrain = createDataPartition(y=training$classe, p=0.7, list=FALSE)
training_data = training[inTrain,]
testing_data = training[-inTrain,]
```

In order to avoid the problem with overfitting, we are going to use 5-fold cross validation technique (obviously, 10 might be better, but we want to save computational time).

```{r, echo=TRUE}
control = trainControl(method = "cv", number = 5)
```

## Models building

We are going to try 3 different models here and compare the accuracy between each model. The models that we are going to try are:

+ decision trees
+ Generalized Boosted Regression Models
+ random forests

First, let's start with the decision trees

```{r, echo=TRUE,fig.height = 9, fig.width = 12}
treeFit = train(classe~., method ="rpart", data=training_data, trControl = control)
plot(treeFit$finalModel, uniform=TRUE, main = "classification tree")
text(treeFit$finalModel, use.n=TRUE, cex = .8)
```

```{r, echo=TRUE}
treePred = predict(treeFit, newdata = testing_data)
treeResult = confusionMatrix(treePred, testing_data$classe)
treeResult$overall["Accuracy"]
```

```{r, echo=TRUE}
treeResult$table
```

We can see that the accuracy for this model is pretty bad, even lower than 50% on the test set. Looking at the confusion matrix, we are doing pretty good at predicting class E. But, we didn't predict anything to be class D.

Let's see how well we could do using generalized boosting method.

```{r, echo=TRUE}
gbmFit = train(classe~., method ="gbm", data=training_data, verbose=FALSE, trControl = control)
gbmPred = predict(gbmFit, newdata = testing_data)
gbmResult = confusionMatrix(gbmPred, testing_data$classe)
gbmResult$overall["Accuracy"]
```
```{r, echo=TRUE}
gbmResult$table
```

Comparing to the accuracy of the model generated by decision trees, Generalized Boosted Regression Model is much better. It could achieve about 99% accuracy on the test set. 

Let's try out the random forest model for fun.

```{r, echo=TRUE}
rfFit = train(classe~., method ="rf", data=training_data, trControl = control)
rfPred = predict(rfFit, newdata = testing_data)
rfResult = confusionMatrix(rfPred, testing_data$classe)
rfResult$overall["Accuracy"]
```
```{r, echo=TRUE}
rfResult$table
```

It looks like that we can still improve the accuracy of the model a bit by using random forest. The accuracy on the test set is about 99.8% which is very high. Therefore, we can expect a very small out-of-sample error (less than ~ 1%).

## Prediction on the validation set

Based on the project's requirement, we will predict the class for the 20 observations in the validation dataset. We will base our result from the random forest model since it has the highest accuracy. Additional, we will also use the genalarized boosting model to check whether we got different outcomes or not.


```{r, echo=TRUE}
rf = predict(rfFit, newdata = validation)
gbm = predict(gbmFit, newdata = validation)
```

```{r, echo=TRUE}
data.frame(rf,gbm)
```

We can see that both models yield the same results on the validation set.
